{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDaRDdCoqtie"
      },
      "source": [
        "# REINFORCE with Baseline as Value Function for CartPole-v1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yr-5A6aZghNA"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "\n",
        "import argparse\n",
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "from IPython.display import HTML\n",
        "import numpy as np\n",
        "from itertools import count\n",
        "from collections import namedtuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yp1Og4NBp7-u"
      },
      "source": [
        "## Environment Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNIZX3kGghNB",
        "outputId": "811f867e-bd83-4fe5-a55e-6353bccf7994"
      },
      "outputs": [],
      "source": [
        "# Set constants for training\n",
        "seed = 9474\n",
        "log_interval = 10\n",
        "gamma = 0.99\n",
        "\n",
        "env = gym.make('CartPole-v1')\n",
        "env.reset(seed=seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])\n",
        "\n",
        "num_i_nodes = 4\n",
        "num_h_nodes = 64\n",
        "num_o_nodes = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQAbkeX9qCyJ"
      },
      "source": [
        "## Neural Networks for Policy and Value Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OP4Zb0RqGsu"
      },
      "source": [
        "### Policy Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EgxavOyKghNC"
      },
      "outputs": [],
      "source": [
        "class Policy(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements the policy network for REINFORCE with baseline.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(Policy, self).__init__()\n",
        "        self.affine1 = nn.Linear(num_i_nodes, num_h_nodes)\n",
        "        self.action_head = nn.Linear(num_h_nodes, num_o_nodes)  # Output layer for actions\n",
        "\n",
        "        # Initialize the weights\n",
        "        self.init_weights()\n",
        "\n",
        "        # Action and reward buffer (unused for REINFORCE)\n",
        "        self.saved_actions = []\n",
        "        self.rewards = []\n",
        "\n",
        "    def init_weights(self):\n",
        "        # Initialize the weights of the layers\n",
        "        nn.init.kaiming_normal_(self.affine1.weight, nonlinearity='relu')\n",
        "        nn.init.constant_(self.affine1.bias, 0)\n",
        "        nn.init.kaiming_normal_(self.action_head.weight, nonlinearity='relu')\n",
        "        nn.init.constant_(self.action_head.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the policy network.\n",
        "        \"\"\"\n",
        "        x = F.relu(self.affine1(x))\n",
        "        action_scores = F.softmax(self.action_head(x), dim=-1)  # Actor: Action probabilities\n",
        "        return action_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMOqDDOXqLkC"
      },
      "source": [
        "### Value Function Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzjJ8yI2ZHYn"
      },
      "outputs": [],
      "source": [
        "# Create NN for value function\n",
        "class ValueFunction(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements the value function network for REINFORCE with baseline.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(ValueFunction, self).__init__()\n",
        "        self.affine1 = nn.Linear(num_i_nodes, num_h_nodes)\n",
        "        self.value_head = nn.Linear(num_h_nodes, 1)    # Output layer for value function\n",
        "\n",
        "        # Initialize the weights\n",
        "        self.init_weights()\n",
        "\n",
        "        # State value buffer (unused for REINFORCE)\n",
        "        self.state_values = []\n",
        "\n",
        "    def init_weights(self):\n",
        "        # Initialize the weights of the linear layers\n",
        "        nn.init.kaiming_normal_(self.affine1.weight, nonlinearity='relu')\n",
        "        nn.init.constant_(self.affine1.bias, 0)\n",
        "        nn.init.kaiming_normal_(self.value_head.weight, nonlinearity='relu')\n",
        "        nn.init.constant_(self.value_head.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the value function network.\n",
        "        \"\"\"\n",
        "        x = F.relu(self.affine1(x))\n",
        "        state_value = self.value_head(x)\n",
        "        return state_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F0tBwysqQWR"
      },
      "source": [
        "## Object Definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_3XqHoMghNC"
      },
      "outputs": [],
      "source": [
        "model = Policy()\n",
        "value_model = ValueFunction()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
        "value_optimizer = optim.Adam(value_model.parameters(), lr=1e-2)\n",
        "eps = np.finfo(np.float32).eps.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9q-yww-qTkc"
      },
      "source": [
        "## Training Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H43IuBvDqXa-"
      },
      "source": [
        "### Action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxnhPP5wghNC",
        "outputId": "b3e396ed-02af-41d6-fe7d-59de8fbbf94e"
      },
      "outputs": [],
      "source": [
        "def select_action(state):\n",
        "    \"\"\"\n",
        "    Selects an action from the policy network given the current state.\n",
        "    \"\"\"\n",
        "    state = torch.from_numpy(state).float()\n",
        "    probs = model(state)\n",
        "    state_value = value_model(state)\n",
        "\n",
        "    # Create a categorical distribution over the list of probabilities of actions\n",
        "    m = Categorical(probs)\n",
        "\n",
        "    # Sample an action using the distribution\n",
        "    action = m.sample()\n",
        "\n",
        "    # Save the log probability and state value in the model's saved_actions\n",
        "    model.saved_actions.append(SavedAction(m.log_prob(action), state_value))\n",
        "\n",
        "    # Return the action to take (left or right)\n",
        "    return action.item(), m.log_prob(action), state_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XD003_sqajA"
      },
      "source": [
        "### Returns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmf5oeL4SKRo"
      },
      "outputs": [],
      "source": [
        "def calculate_returns(rewards, discount_factor, normalize = False):\n",
        "\n",
        "    returns = []\n",
        "    R = 0\n",
        "\n",
        "    for r in reversed(rewards):\n",
        "        R = r + R * discount_factor\n",
        "        returns.insert(0, R)\n",
        "\n",
        "    # returns = torch.tensor(returns)\n",
        "\n",
        "    if normalize:\n",
        "        returns = (returns - returns.mean()) / returns.std()\n",
        "\n",
        "    return returns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1I_qqjUDqceq"
      },
      "source": [
        "### Completion and Updation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PoSHGfvR2S2T"
      },
      "outputs": [],
      "source": [
        "def finish_episode():\n",
        "    policy_loss = []\n",
        "    value_loss = []\n",
        "    returns = calculate_returns(model.rewards, gamma)\n",
        "    returns = torch.tensor(returns)\n",
        "    for saved_action, G, state_value in zip(model.saved_actions, returns, value_model.state_values):\n",
        "        log_prob, _ = saved_action\n",
        "        advantage = G - state_value\n",
        "        policy_loss.append(-log_prob * advantage)\n",
        "        value_loss.append(F.smooth_l1_loss(state_value, G))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    policy_loss = torch.stack(policy_loss).sum()\n",
        "    policy_loss.backward(retain_graph=True)\n",
        "    optimizer.step()\n",
        "\n",
        "    value_optimizer.zero_grad()\n",
        "    value_loss = torch.stack(value_loss).sum()\n",
        "    value_loss.backward(retain_graph=True)\n",
        "    value_optimizer.step()\n",
        "\n",
        "    value_loss = value_loss.detach()\n",
        "\n",
        "    del model.rewards[:]\n",
        "    del model.saved_actions[:]\n",
        "    del value_model.state_values[:]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1KsOyBmqiuX"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdUB0QoIghNC"
      },
      "outputs": [],
      "source": [
        "def train(episodes):\n",
        "    running_reward = 10  # Initialize running reward\n",
        "\n",
        "    # Keep track of the rewards for plotting\n",
        "    reward_history = []\n",
        "\n",
        "    # Run infinitely many episodes\n",
        "    for episode in range(episodes):\n",
        "\n",
        "        # Reset environment and episode reward\n",
        "        state = env.reset()\n",
        "        ep_reward = 0\n",
        "\n",
        "        # For each episode, only run 9999 steps to avoid infinite loop\n",
        "        for t in range(1, 10000):\n",
        "\n",
        "            # Select action\n",
        "            action, log_prob, state_value = select_action(state)\n",
        "\n",
        "            # Take action\n",
        "            state, reward, done, _ = env.step(action)\n",
        "\n",
        "            # Save reward and state value\n",
        "            model.rewards.append(reward)\n",
        "            value_model.state_values.append(state_value)\n",
        "\n",
        "            ep_reward += reward\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # Update cumulative reward\n",
        "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
        "\n",
        "        # Store the reward\n",
        "        reward_history.append(running_reward)\n",
        "\n",
        "        # Perform policy update\n",
        "        finish_episode()\n",
        "\n",
        "        # Log results\n",
        "        if episode % log_interval == 0:\n",
        "            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
        "                episode, ep_reward, running_reward))\n",
        "\n",
        "        # Check if the problem is solved\n",
        "        if running_reward > env.spec.reward_threshold:\n",
        "            print(\"Solved! Running reward is now {} and the last episode ({}) runs to {} time steps!\".format(running_reward, episode, t))\n",
        "            break\n",
        "\n",
        "    return reward_history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HQvCiAzKqk8L"
      },
      "source": [
        "## Commence Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jm8z_1TLghND",
        "outputId": "1d4d1e82-101d-4424-cb65-71d3e33c7572"
      },
      "outputs": [],
      "source": [
        "# Best hyperparameter settings\n",
        "num_expts = 5\n",
        "num_episodes = 2000\n",
        "lr = 1e-2\n",
        "num_h_nodes = 64\n",
        "log_interval = 100\n",
        "\n",
        "reward_histories = []\n",
        "\n",
        "mean_rewards = []\n",
        "std_rewards = []\n",
        "\n",
        "for i in range(num_expts):\n",
        "    model = Policy()\n",
        "    value_model = ValueFunction()\n",
        "    optimizer = optim.Adam(model.parameters(), lr)\n",
        "    value_optimizer = optim.Adam(value_model.parameters(), lr)\n",
        "    reward_history = train(num_episodes)\n",
        "    while len(reward_history) < num_episodes:\n",
        "        reward_history.append(env.spec.reward_threshold)\n",
        "    reward_histories.append(reward_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o83xWwh3ajW7"
      },
      "outputs": [],
      "source": [
        "reward_histories = np.array(reward_histories)\n",
        "\n",
        "mean_rewards = np.mean(reward_histories, axis=0)\n",
        "std_rewards = np.std(reward_histories, axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQcXL98BruAO"
      },
      "source": [
        "## Reward History Plot During Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "8ykwEUodSzsB",
        "outputId": "dafe5ae7-348f-415e-edb9-83fd6203485e"
      },
      "outputs": [],
      "source": [
        "# Plot the reward history\n",
        "\n",
        "plt.plot(mean_rewards)\n",
        "plt.fill_between(range(len(mean_rewards)), mean_rewards + std_rewards, mean_rewards - std_rewards, alpha=0.5)\n",
        "plt.title('REINFORCE with Baseline CartPole-v1')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Reward')\n",
        "plt.legend(['Running Reward'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-y2b0QAqpka"
      },
      "source": [
        "## Save Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhRvdREXs90I"
      },
      "outputs": [],
      "source": [
        "torch.save(model, 'REINFORCEB_CartPole_v1.pth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jw8mJWHEgntm"
      },
      "outputs": [],
      "source": [
        "# Save the mean and standard deviation rewards to a csv file\n",
        "np.savetxt('REINFORCEB_CartPole_v1_mean.csv', mean_rewards, delimiter=',')\n",
        "np.savetxt('REINFORCEB_CartPole_v1_std.csv', std_rewards, delimiter=',')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHgSD4Qh_2ci"
      },
      "source": [
        "## Evaluation of the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3mrVbxZ_x4M"
      },
      "outputs": [],
      "source": [
        "# Evaluate the policy using total regret\n",
        "\n",
        "def calculate_total_regret(episodes):\n",
        "    \"\"\"\n",
        "    Evaluate the policy using total regret.\n",
        "    \"\"\"\n",
        "    total_regret = 0\n",
        "\n",
        "    # Use reward history to calculate total regret\n",
        "    for reward in reward_history:\n",
        "        total_regret += env.spec.reward_threshold - reward\n",
        "\n",
        "    return total_regret"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzUDGHAM-6Ju"
      },
      "source": [
        "## Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        },
        "id": "22IN3rk6-8UM",
        "outputId": "c7d4fbd6-d2cd-4806-b6a4-7e9c34680fe4"
      },
      "outputs": [],
      "source": [
        "# Hyperparameter tuning\n",
        "\n",
        "# Set the hyperparameters\n",
        "hyperparameters = {\n",
        "    'lr': [1e-2, 1e-3, 1e-4],\n",
        "    'num_h_nodes': [32, 64, 128]\n",
        "}\n",
        "\n",
        "env.reset(seed=seed)\n",
        "torch.manual_seed(seed)\n",
        "log_interval = 100\n",
        "episodes = 2000\n",
        "best_total_regret = float('inf')\n",
        "best_hyperparameters = {}\n",
        "best_episodes = 2000\n",
        "regret_storage = []\n",
        "reward_history_storage = np.zeros((len(hyperparameters['lr']), len(hyperparameters['num_h_nodes']), episodes))\n",
        "\n",
        "# Loop through the hyperparameters\n",
        "for lr in hyperparameters['lr']:\n",
        "    for num_h_nodes in hyperparameters['num_h_nodes']:\n",
        "\n",
        "        # Set the hyperparameters\n",
        "        model = Policy()\n",
        "        value_model = ValueFunction()\n",
        "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "        value_optimizer = optim.Adam(value_model.parameters(), lr=lr)\n",
        "\n",
        "        # Train the model\n",
        "        reward_history = train(episodes)\n",
        "        len_episodes = len(reward_history)\n",
        "        reward_history += [env.spec.reward_threshold] * (episodes - len(reward_history))\n",
        "        reward_history_storage[hyperparameters['lr'].index(lr), hyperparameters['num_h_nodes'].index(num_h_nodes), :] = reward_history\n",
        "\n",
        "        # Calculate the total regret\n",
        "        total_regret = calculate_total_regret(episodes)\n",
        "\n",
        "        # Store the total regret\n",
        "        regret_storage.append([lr, num_h_nodes, total_regret])\n",
        "\n",
        "        # Check if this is the best total regret\n",
        "        if total_regret < best_total_regret:\n",
        "            best_total_regret = total_regret\n",
        "            best_hyperparameters = {'lr': lr, 'num_h_nodes': num_h_nodes}\n",
        "            best_episodes = len_episodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "im8rvexCBwAf",
        "outputId": "1677a177-0323-4dd2-e30a-23508bcef9a2"
      },
      "outputs": [],
      "source": [
        "# Print the best hyperparameters\n",
        "print('Best hyperparameters:', best_hyperparameters)\n",
        "print('Best total regret:', best_total_regret)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "cmDEHvEHBxe8",
        "outputId": "33b801f8-0fc4-48a5-c1fd-2bea91ce7278"
      },
      "outputs": [],
      "source": [
        "# Plot the reward history for the best hyperparameters\n",
        "\n",
        "plt.plot(reward_history_storage[hyperparameters['lr'].index(best_hyperparameters['lr']), hyperparameters['num_h_nodes'].index(best_hyperparameters['num_h_nodes']), :best_episodes])\n",
        "plt.title('Reward history for the best hyperparameters')\n",
        "plt.xlabel('Episode')\n",
        "plt.ylabel('Average Reward')\n",
        "plt.legend(['lr: {}, num_h_nodes: {}'.format(best_hyperparameters['lr'], best_hyperparameters['num_h_nodes'])])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k775s0DsB0KX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
