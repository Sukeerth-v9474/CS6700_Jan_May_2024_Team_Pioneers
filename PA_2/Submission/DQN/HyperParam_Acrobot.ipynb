{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 567
        },
        "id": "pATBIzOq__gK",
        "outputId": "4e9c74be-5877-4a69-a2d0-644fd0b2d06b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (67.7.2)\n",
            "Collecting setuptools\n",
            "  Downloading setuptools-69.2.0-py3-none-any.whl (821 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.5/821.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: setuptools\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 67.7.2\n",
            "    Uninstalling setuptools-67.7.2:\n",
            "      Successfully uninstalled setuptools-67.7.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed setuptools-69.2.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack",
                  "pkg_resources",
                  "setuptools"
                ]
              },
              "id": "d59011dbc20b4cb49c2be3297201cb12"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym[classic_control] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[classic_control]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[classic_control]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[classic_control]) (0.0.8)\n",
            "Collecting pygame==2.1.0 (from gym[classic_control])\n",
            "  Downloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pygame\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.5.2\n",
            "    Uninstalling pygame-2.5.2:\n",
            "      Successfully uninstalled pygame-2.5.2\n",
            "Successfully installed pygame-2.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install cmake > /dev/null 2>&1\n",
        "!pip install --upgrade setuptools 2>&1\n",
        "!pip install ez_setup > /dev/null 2>&1\n",
        "!pip install gym[atari] > /dev/null 2>&1\n",
        "!pip install git+https://github.com/tensorflow/docs > /dev/null 2>&1\n",
        "!pip install gym[classic_control]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ma0dkJEF_R-Y",
        "outputId": "1b71f4c5-703e-42c4-d123-d0681f7c094a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if (distutils.version.LooseVersion(tf.__version__) <\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from collections import namedtuple, deque\n",
        "import torch.optim as optim\n",
        "import datetime\n",
        "import gym\n",
        "from gym.wrappers.record_video import RecordVideo\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML\n",
        "from pyvirtualdisplay import Display\n",
        "import tensorflow as tf\n",
        "from IPython import display as ipythondisplay\n",
        "from PIL import Image\n",
        "import tensorflow_probability as tfp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-6BgqE8_rBr",
        "outputId": "84fa31f0-d63d-45d6-b799-1775323a2910"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.9969729  -0.0777493   0.9962257   0.08680018  0.09011854  0.0894108 ]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "'''\n",
        "Bunch of Hyper parameters (Which you might have to tune later)\n",
        "'''\n",
        "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
        "BATCH_SIZE = 64         # minibatch size\n",
        "GAMMA = 0.99            # discount factor\n",
        "LR = 5e-5             # learning rate\n",
        "\n",
        "env = gym.make('Acrobot-v1')\n",
        "state = env.reset()\n",
        "print(state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mvMchDBAZWW",
        "outputId": "8f8e29c5-51d6-4632-dd12-7bcf65e8d259"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([[ 0.1462],\n",
            "        [-0.1963]], grad_fn=<AddmmBackward0>), tensor([[ 0.4298, -0.0550, -0.5994],\n",
            "        [ 1.2368,  0.3832, -1.6038]], grad_fn=<AddmmBackward0>))\n"
          ]
        }
      ],
      "source": [
        "class QNetwork1(nn.Module):\n",
        "\n",
        "  def __init__(self, state_size, action_size, hl_size):\n",
        "    super(QNetwork1,self).__init__()\n",
        "    self.input_layer = nn.Linear(state_size,hl_size)\n",
        "    self.value_layer = nn.Linear(hl_size,1)\n",
        "    self.advantage_layer = nn.Linear(hl_size,action_size)\n",
        "\n",
        "  def forward(self,state):\n",
        "    x = F.relu(self.input_layer(state))\n",
        "    value = self.value_layer(x)\n",
        "    advantage = self.advantage_layer(x)\n",
        "    return value , advantage\n",
        "\n",
        "\n",
        "network = QNetwork1(6,3,2)\n",
        "print(network.forward(torch.tensor([[1,1,1,1,1,4],[1,2,3,4,5,4]],dtype=torch.float32)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKo3dlhkLyJt"
      },
      "source": [
        "## Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "IgB9s_x7Dd-0"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "from collections import deque, namedtuple\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "class ReplayBuffer:\n",
        "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
        "\n",
        "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
        "        \"\"\"Initialize a ReplayBuffer object.\n",
        "\n",
        "        Params\n",
        "        ======\n",
        "            action_size (int): dimension of each action\n",
        "            buffer_size (int): maximum size of buffer\n",
        "            batch_size (int): size of each training batch\n",
        "            seed (int): random seed\n",
        "        \"\"\"\n",
        "        self.action_size = action_size\n",
        "        self.memory = deque(maxlen=buffer_size)\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "        self.seed = random.seed(seed)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Add a new experience to memory.\"\"\"\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "\n",
        "    def sample(self):\n",
        "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)\n",
        "\n",
        "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
        "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
        "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
        "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
        "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
        "\n",
        "        return (states, actions, rewards, next_states, dones)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the current size of internal memory.\"\"\"\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aq3_0wDnON2F"
      },
      "source": [
        "#DQN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wz9uW2suPBiG"
      },
      "source": [
        "##mean(A)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IytbQZxmMC6G"
      },
      "outputs": [],
      "source": [
        "class TutorialAgent():\n",
        "\n",
        "    def __init__(self, state_size, action_size, seed, hl_size = 64, batch_size = 32, lr = 1e-5):\n",
        "\n",
        "        ''' Agent Environment Interaction '''\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.seed = random.seed(seed)\n",
        "        self.batch_size = batch_size\n",
        "        ''' Q-Network '''\n",
        "        self.qnetwork_local = QNetwork1(state_size, action_size, hl_size = hl_size).to(device)\n",
        "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=lr)\n",
        "\n",
        "        ''' Replay memory '''\n",
        "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, batch_size, seed)\n",
        "\n",
        "        ''' Initialize time step (for updating every UPDATE_EVERY steps)           -Needed for Q Targets '''\n",
        "        self.t_step = 0\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "\n",
        "        ''' Save experience in replay memory '''\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "        ''' If enough samples are available in memory, get random subset and learn '''\n",
        "        if len(self.memory) >= self.batch_size:\n",
        "            experiences = self.memory.sample()\n",
        "            self.learn(experiences, GAMMA)\n",
        "\n",
        "\n",
        "    def act(self, state, eps=0.5):\n",
        "\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        self.qnetwork_local.eval()\n",
        "        with torch.no_grad():\n",
        "            _,action_values = self.qnetwork_local(state)\n",
        "        self.qnetwork_local.train()\n",
        "\n",
        "        ''' Epsilon-greedy action selection (Already Present) '''\n",
        "        if random.random() > eps:\n",
        "            return torch.argmax(action_values.to('cpu'),dim=1).item()\n",
        "        else:\n",
        "            return random.choice(np.arange(self.action_size))\n",
        "\n",
        "    def learn(self, experiences, gamma):\n",
        "        \"\"\" +E EXPERIENCE REPLAY PRESENT \"\"\"\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "        ''' Get max predicted Q values (for next states) from target model'''\n",
        "        V_targets_next, A_targets_next = self.qnetwork_local(next_states)\n",
        "        Q_targets_next = torch.add(V_targets_next, A_targets_next - A_targets_next.mean(dim=1, keepdim=True)).detach().max(1)[0].unsqueeze(1)\n",
        "\n",
        "        ''' Compute Q targets for current states '''\n",
        "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
        "        ''' Get expected Q values from local model '''\n",
        "        V_targets, A_targets = self.qnetwork_local(states)\n",
        "        Q_expected = torch.add(V_targets, A_targets - A_targets.mean(dim=1, keepdim=True)).gather(1, actions)\n",
        "\n",
        "        ''' Compute loss '''\n",
        "        loss = F.mse_loss(Q_expected, Q_targets)\n",
        "\n",
        "        ''' Minimize the loss '''\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        ''' Gradiant Clipping '''\n",
        "        \"\"\" +T TRUNCATION PRESENT \"\"\"\n",
        "        for param in self.qnetwork_local.parameters():\n",
        "            param.grad.data.clamp_(-1, 1)\n",
        "\n",
        "        self.optimizer.step()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3q3lCpuzMCwO"
      },
      "outputs": [],
      "source": [
        "''' Defining DQN Algorithm '''\n",
        "\n",
        "state_shape = env.observation_space.shape[0]\n",
        "action_shape = env.action_space.n\n",
        "\n",
        "\n",
        "def dqn(n_episodes=500, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995,print_reward = False):\n",
        "\n",
        "    scores_window = deque(maxlen=100)\n",
        "    ''' last 100 scores for checking if the avg is more than 195 '''\n",
        "\n",
        "    eps = eps_start\n",
        "    ''' initialize epsilon '''\n",
        "    total_reward = []\n",
        "    for i_episode in range(1, n_episodes+1):\n",
        "        state = env.reset()\n",
        "        score = 0\n",
        "        for t in range(max_t):\n",
        "            action = agent.act(state, eps)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            score += reward\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        scores_window.append(score)\n",
        "        total_reward.append(score)\n",
        "        eps = max(eps_end, eps_decay*eps)\n",
        "        ''' decrease epsilon '''\n",
        "        # print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, score), end=\"\")\n",
        "        if i_episode % 10 == 0 and print_reward == True:\n",
        "           print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "        if np.mean(scores_window)>=-110.0:\n",
        "           print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "           break\n",
        "    return total_reward\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YW2VQGvNPBTK"
      },
      "source": [
        "##Batch size 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vutOz00sMP-h"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = [32]\n",
        "LR = [5e-4,1e-4,5e-5]\n",
        "HL_size = [8,16,32]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJyZ8v8iMP8L",
        "outputId": "4f988295-b53b-400f-8550-d64148310bbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Copmleted iteration 1\n",
            "Copmleted iteration 2\n",
            "Copmleted iteration 3\n",
            "\n",
            "Environment solved in 326 episodes!\tAverage Score: -109.83\n",
            "Copmleted iteration 4\n",
            "Copmleted iteration 5\n",
            "Copmleted iteration 6\n",
            "Copmleted iteration 7\n",
            "Copmleted iteration 8\n",
            "Copmleted iteration 9\n"
          ]
        }
      ],
      "source": [
        "reward_max = []\n",
        "i = 0\n",
        "for batch_size in BATCH_SIZE:\n",
        "  for lr in LR:\n",
        "    for hl_size in HL_size:\n",
        "      agent = TutorialAgent(state_size=state_shape,action_size = action_shape,seed = 0,hl_size = hl_size , batch_size = batch_size , lr = lr)\n",
        "      reward_max.append(dqn())\n",
        "      i+=1\n",
        "      print(f'Copmleted iteration {i}')\n",
        "min_size = min([len(i) for i in reward_max])\n",
        "reward_max = np.array([i[:min_size] for i in reward_max]).reshape(len(LR),len(HL_size),-1)\n",
        "np.save('HP_32_mean.npy',reward_max)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEL5x47KPJ_X"
      },
      "source": [
        "##Batch size 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qnDtZm0wPJ_X"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = [64]\n",
        "LR = [5e-4,1e-4,5e-5]\n",
        "HL_size = [8,16,32]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "epshug_7PJ_X",
        "outputId": "5c8ddc16-0977-4e77-ad30-51db109c8459"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Copmleted iteration 1\n",
            "Copmleted iteration 2\n",
            "Copmleted iteration 3\n",
            "\n",
            "Environment solved in 358 episodes!\tAverage Score: -109.07\n",
            "Copmleted iteration 4\n",
            "Copmleted iteration 5\n",
            "\n",
            "Environment solved in 431 episodes!\tAverage Score: -109.90\n",
            "Copmleted iteration 6\n",
            "\n",
            "Environment solved in 450 episodes!\tAverage Score: -109.93\n",
            "Copmleted iteration 7\n",
            "\n",
            "Environment solved in 308 episodes!\tAverage Score: -109.73\n",
            "Copmleted iteration 8\n",
            "\n",
            "Environment solved in 369 episodes!\tAverage Score: -109.86\n",
            "Copmleted iteration 9\n"
          ]
        }
      ],
      "source": [
        "reward_max = []\n",
        "i = 0\n",
        "for batch_size in BATCH_SIZE:\n",
        "  for lr in LR:\n",
        "    for hl_size in HL_size:\n",
        "      agent = TutorialAgent(state_size=state_shape,action_size = action_shape,seed = 0,hl_size = hl_size , batch_size = batch_size , lr = lr)\n",
        "      reward_max.append(dqn())\n",
        "      i+=1\n",
        "      print(f'Copmleted iteration {i}')\n",
        "min_size = min([len(i) for i in reward_max])\n",
        "reward_max = np.array([i[:min_size] for i in reward_max]).reshape(len(LR),len(HL_size),-1)\n",
        "np.save('HP_64_mean.npy',reward_max)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rZrcx3KPKeI"
      },
      "source": [
        "##Batch size 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqV2f46xPKeI"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = [128]\n",
        "LR = [5e-4,1e-4,5e-5]\n",
        "HL_size = [8,16,32]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49h6ik9xPKeJ",
        "outputId": "8b17c86f-4436-4a3b-95bc-8e6c9ac02e04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Copmleted iteration 1\n",
            "\n",
            "Environment solved in 450 episodes!\tAverage Score: -109.30\n",
            "Copmleted iteration 2\n",
            "Copmleted iteration 3\n",
            "Copmleted iteration 4\n",
            "\n",
            "Environment solved in 351 episodes!\tAverage Score: -109.79\n",
            "Copmleted iteration 5\n",
            "\n",
            "Environment solved in 358 episodes!\tAverage Score: -109.88\n",
            "Copmleted iteration 6\n",
            "\n",
            "Environment solved in 321 episodes!\tAverage Score: -109.95\n",
            "Copmleted iteration 7\n",
            "Copmleted iteration 8\n",
            "Copmleted iteration 9\n"
          ]
        }
      ],
      "source": [
        "reward_max = []\n",
        "i = 0\n",
        "for batch_size in BATCH_SIZE:\n",
        "  for lr in LR:\n",
        "    for hl_size in HL_size:\n",
        "      agent = TutorialAgent(state_size=state_shape,action_size = action_shape,seed = 0,hl_size = hl_size , batch_size = batch_size , lr = lr)\n",
        "      reward_max.append(dqn())\n",
        "      i+=1\n",
        "      print(f'Copmleted iteration {i}')\n",
        "min_size = min([len(i) for i in reward_max])\n",
        "reward_max = np.array([i[:min_size] for i in reward_max]).reshape(len(LR),len(HL_size),-1)\n",
        "np.save('HP_128_mean.npy',reward_max)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5c2CuKSKMP5u"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfnYQD4FMP3S"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLVQ1S7eSZs_"
      },
      "source": [
        "##max(A)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "pJVhhqC9SZtG"
      },
      "outputs": [],
      "source": [
        "class TutorialAgent():\n",
        "\n",
        "    def __init__(self, state_size, action_size, seed, hl_size = 64, batch_size = 32, lr = 1e-5):\n",
        "\n",
        "        ''' Agent Environment Interaction '''\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.seed = random.seed(seed)\n",
        "        self.lr = lr\n",
        "        self.batch_size = batch_size\n",
        "        ''' Q-Network '''\n",
        "        self.qnetwork_local = QNetwork1(state_size, action_size, hl_size = hl_size).to(device)\n",
        "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=self.lr)\n",
        "\n",
        "        ''' Replay memory '''\n",
        "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, batch_size, seed)\n",
        "\n",
        "\n",
        "    def step(self, state, action, reward, next_state, done):\n",
        "\n",
        "        ''' Save experience in replay memory '''\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "\n",
        "        ''' If enough samples are available in memory, get random subset and learn '''\n",
        "        if len(self.memory) >= self.batch_size:\n",
        "            experiences = self.memory.sample()\n",
        "            self.learn(experiences, GAMMA)\n",
        "\n",
        "\n",
        "    def act(self, state, eps=0.5):\n",
        "\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        self.qnetwork_local.eval()\n",
        "        with torch.no_grad():\n",
        "            _,action_values = self.qnetwork_local(state)\n",
        "        self.qnetwork_local.train()\n",
        "\n",
        "        ''' Epsilon-greedy action selection (Already Present) '''\n",
        "        if random.random() > eps:\n",
        "            return torch.argmax(action_values,dim=1).item()\n",
        "        else:\n",
        "            return random.choice(np.arange(self.action_size))\n",
        "\n",
        "    def learn(self, experiences, gamma):\n",
        "        \"\"\" +E EXPERIENCE REPLAY PRESENT \"\"\"\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "        ''' Get max predicted Q values (for next states) from target model'''\n",
        "        V_targets_next, A_targets_next = self.qnetwork_local(next_states)\n",
        "        Q_targets_next = torch.add(V_targets_next, A_targets_next - torch.max(A_targets_next, dim=1).values.unsqueeze(1)).detach().max(1)[0].unsqueeze(1)\n",
        "\n",
        "        ''' Compute Q targets for current states '''\n",
        "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
        "        ''' Get expected Q values from local model '''\n",
        "        V_targets, A_targets = self.qnetwork_local(states)\n",
        "        Q_expected = torch.add(V_targets, A_targets - torch.max(A_targets, dim=1).values.unsqueeze(1)).gather(1, actions)\n",
        "\n",
        "        ''' Compute loss '''\n",
        "        loss = F.mse_loss(Q_expected, Q_targets)\n",
        "\n",
        "        ''' Minimize the loss '''\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        ''' Gradiant Clipping '''\n",
        "        \"\"\" +T TRUNCATION PRESENT \"\"\"\n",
        "        for param in self.qnetwork_local.parameters():\n",
        "            param.grad.data.clamp_(-1, 1)\n",
        "\n",
        "        self.optimizer.step()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qgp1p51rSZtH",
        "outputId": "21955ecf-c9c7-48c0-8b48-2500c9a2ec75"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Trial run to check if algorithm runs and saves the data '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "''' Defining DQN Algorithm '''\n",
        "\n",
        "state_shape = env.observation_space.shape[0]\n",
        "action_shape = env.action_space.n\n",
        "\n",
        "\n",
        "def dqn(n_episodes=500, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995,print_reward = False):\n",
        "\n",
        "    scores_window = deque(maxlen=100)\n",
        "    ''' last 100 scores for checking if the avg is more than 195 '''\n",
        "    total_reward = []\n",
        "    eps = eps_start\n",
        "    ''' initialize epsilon '''\n",
        "\n",
        "    for i_episode in range(1, n_episodes+1):\n",
        "        state = env.reset()\n",
        "        score = 0\n",
        "        for t in range(max_t):\n",
        "            action = agent.act(state, eps)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.step(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            score += reward\n",
        "            if done:\n",
        "                break\n",
        "        total_reward.append(score)\n",
        "        scores_window.append(score)\n",
        "\n",
        "        eps = max(eps_end, eps_decay*eps)\n",
        "        ''' decrease epsilon '''\n",
        "\n",
        "        if i_episode % 10 == 0 and print_reward == True:\n",
        "           print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "        if np.mean(scores_window)>=-110:\n",
        "           print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
        "           break\n",
        "    return total_reward\n",
        "\n",
        "''' Trial run to check if algorithm runs and saves the data '''\n",
        "\n",
        "# begin_time = datetime.datetime.now()\n",
        "\n",
        "# agent = TutorialAgent(state_size=state_shape,action_size = action_shape,seed = 0)\n",
        "# dqn(print_reward = True)\n",
        "\n",
        "# time_taken = datetime.datetime.now() - begin_time\n",
        "\n",
        "# print(time_taken)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teBZXGWfnq1h"
      },
      "source": [
        "##Batch size 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Qbo6-C-nq1i"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = [32]\n",
        "LR = [5e-4,1e-4,5e-5]\n",
        "HL_size = [8,16,32]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49c5bfce-8753-4a9d-8055-e349bec926b1",
        "id": "b_1RwND4nq1j"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copmleted iteration 1\n",
            "Copmleted iteration 2\n",
            "Copmleted iteration 3\n",
            "Copmleted iteration 4\n",
            "Copmleted iteration 5\n",
            "Copmleted iteration 6\n",
            "Copmleted iteration 7\n",
            "Copmleted iteration 8\n",
            "Copmleted iteration 9\n"
          ]
        }
      ],
      "source": [
        "reward_max = []\n",
        "i = 0\n",
        "for batch_size in BATCH_SIZE:\n",
        "  for lr in LR:\n",
        "    for hl_size in HL_size:\n",
        "      agent = TutorialAgent(state_size=state_shape,action_size = action_shape,seed = 0,hl_size = hl_size , batch_size = batch_size , lr = lr)\n",
        "      reward_max.append(dqn())\n",
        "      i+=1\n",
        "      print(f'Copmleted iteration {i}')\n",
        "min_size = min([len(i) for i in reward_max])\n",
        "reward_max = np.array([i[:min_size] for i in reward_max]).reshape(len(LR),len(HL_size),-1)\n",
        "np.save('HP_32_max.npy',reward_max)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJvCB4pbnq1j"
      },
      "source": [
        "##Batch size 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01ca7be6-3c96-4250-f5d9-8331d2d627e3",
        "id": "vqSARA2Vnq1k"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = [64]\n",
        "LR = [5e-4,1e-4,5e-5]\n",
        "HL_size = [8,16,32]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4eea86e3-35ab-4fb9-97bb-d4e0a167760c",
        "id": "R6cZ7pcnnq1k"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Copmleted iteration 1\n",
            "Copmleted iteration 2\n",
            "Copmleted iteration 3\n",
            "Copmleted iteration 4\n",
            "Copmleted iteration 5\n",
            "Copmleted iteration 6\n",
            "Copmleted iteration 7\n",
            "Copmleted iteration 8\n",
            "Copmleted iteration 9\n"
          ]
        }
      ],
      "source": [
        "reward_max = []\n",
        "i = 0\n",
        "for batch_size in BATCH_SIZE:\n",
        "  for lr in LR:\n",
        "    for hl_size in HL_size:\n",
        "      agent = TutorialAgent(state_size=state_shape,action_size = action_shape,seed = 0,hl_size = hl_size , batch_size = batch_size , lr = lr)\n",
        "      reward_max.append(dqn())\n",
        "      i+=1\n",
        "      print(f'Copmleted iteration {i}')\n",
        "min_size = min([len(i) for i in reward_max])\n",
        "reward_max = np.array([i[:min_size] for i in reward_max]).reshape(len(LR),len(HL_size),-1)\n",
        "np.save('HP_64_max.npy',reward_max)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9TZuvm_nfJX"
      },
      "source": [
        "##Batch size 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgP5nG9MnfJZ"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = [128]\n",
        "LR = [5e-4,1e-4,5e-5]\n",
        "HL_size = [8,16,32]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4576f408-852a-473a-ffc9-c3337a344195",
        "id": "PWVM_V94nfJZ"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Copmleted iteration 1\n",
            "Copmleted iteration 2\n",
            "Copmleted iteration 3\n",
            "\n",
            "Environment solved in 441 episodes!\tAverage Score: -109.65\n",
            "Copmleted iteration 4\n",
            "\n",
            "Environment solved in 471 episodes!\tAverage Score: -109.44\n",
            "Copmleted iteration 5\n",
            "Copmleted iteration 6\n",
            "\n",
            "Environment solved in 454 episodes!\tAverage Score: -109.53\n",
            "Copmleted iteration 7\n",
            "Copmleted iteration 8\n",
            "Copmleted iteration 9\n"
          ]
        }
      ],
      "source": [
        "reward_max = []\n",
        "i = 0\n",
        "for batch_size in BATCH_SIZE:\n",
        "  for lr in LR:\n",
        "    for hl_size in HL_size:\n",
        "      agent = TutorialAgent(state_size=state_shape,action_size = action_shape,seed = 0,hl_size = hl_size , batch_size = batch_size , lr = lr)\n",
        "      reward_max.append(dqn())\n",
        "      i+=1\n",
        "      print(f'Copmleted iteration {i}')\n",
        "min_size = min([len(i) for i in reward_max])\n",
        "reward_max = np.array([i[:min_size] for i in reward_max]).reshape(len(LR),len(HL_size),-1)\n",
        "np.save('HP_128_max.npy',reward_max)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MblZid7JmbPm"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}