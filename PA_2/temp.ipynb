{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "\n",
    "import argparse\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "from IPython.display import HTML\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set constants for training\n",
    "seed = 9474\n",
    "log_interval = 10\n",
    "gamma = 0.99\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "env.reset(seed=seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])\n",
    "\n",
    "num_i_nodes = 4\n",
    "num_h_nodes = 64\n",
    "num_o_nodes = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the policy network for REINFORCE with baseline.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(num_i_nodes, num_h_nodes)\n",
    "        self.action_head = nn.Linear(num_h_nodes, num_o_nodes)  # Output layer for actions\n",
    "\n",
    "        # Initialize the weights\n",
    "        self.init_weights()\n",
    "\n",
    "        # Action and reward buffer (unused for REINFORCE)\n",
    "        self.saved_actions = []\n",
    "        self.rewards = []\n",
    "\n",
    "    def init_weights(self):\n",
    "        # Initialize the weights of the linear layers\n",
    "        nn.init.kaiming_normal_(self.affine1.weight, nonlinearity='relu')\n",
    "        nn.init.constant_(self.affine1.bias, 0)\n",
    "        nn.init.kaiming_normal_(self.action_head.weight, nonlinearity='relu')\n",
    "        nn.init.constant_(self.action_head.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the policy network.\n",
    "        \"\"\"\n",
    "        x = F.relu(self.affine1(x))\n",
    "        action_scores = F.softmax(self.action_head(x), dim=-1)  # Actor: Action probabilities\n",
    "        return action_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create NN for value function\n",
    "class ValueFunction(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the value function network for REINFORCE with baseline.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(ValueFunction, self).__init__()\n",
    "        self.affine1 = nn.Linear(num_i_nodes, num_h_nodes)\n",
    "        self.value_head = nn.Linear(num_h_nodes, 1)    # Output layer for value function\n",
    "\n",
    "        # Initialize the weights\n",
    "        self.init_weights()\n",
    "\n",
    "        # State value buffer (unused for REINFORCE)\n",
    "        self.state_values = []\n",
    "\n",
    "    def init_weights(self):\n",
    "        # Initialize the weights of the linear layers\n",
    "        nn.init.kaiming_normal_(self.affine1.weight, nonlinearity='relu')\n",
    "        nn.init.constant_(self.affine1.bias, 0)\n",
    "        nn.init.kaiming_normal_(self.value_head.weight, nonlinearity='relu')\n",
    "        nn.init.constant_(self.value_head.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the value function network.\n",
    "        \"\"\"\n",
    "        x = F.relu(self.affine1(x))\n",
    "        state_value = self.value_head(x)\n",
    "        return state_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Policy()\n",
    "value_model = ValueFunction()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "value_optimizer = optim.Adam(value_model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state):\n",
    "    \"\"\"\n",
    "    Select an action based on the current state.\n",
    "    \"\"\"\n",
    "    state = torch.from_numpy(state).float()\n",
    "    probs = model(state)\n",
    "    state_value = value_model(state)\n",
    "    m = Categorical(probs)\n",
    "    action = m.sample()\n",
    "    model.saved_actions.append(SavedAction(m.log_prob(action), state_value))\n",
    "    return action.item(), m.log_prob(action), state_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_returns(rewards, gamma):\n",
    "    \"\"\"\n",
    "    Calculate the discounted returns for a given episode.\n",
    "    \"\"\"\n",
    "    returns = []\n",
    "    R = 0\n",
    "    for r in rewards[::-1]:\n",
    "        R = r + gamma * R\n",
    "        returns.insert(0, R)\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finish_episode():\n",
    "    \"\"\"\n",
    "    Finish the episode and update the policy.\n",
    "    \"\"\"\n",
    "    R = 0\n",
    "    policy_loss = []\n",
    "    value_loss = []\n",
    "    returns = calculate_returns(model.rewards, gamma)\n",
    "    returns = torch.tensor(returns)\n",
    "    # returns = (returns - returns.mean()) / (returns.std() + np.finfo(np.float32).eps.item())\n",
    "\n",
    "    for saved_action, G, state_value in zip(model.saved_actions, returns, value_model.state_values):\n",
    "        log_prob, _ = saved_action\n",
    "        advantage = G - state_value\n",
    "        policy_loss.append(-log_prob * advantage)\n",
    "        value_loss.append(F.smooth_l1_loss(state_value, G))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    value_optimizer.zero_grad()\n",
    "    policy_loss = torch.stack(policy_loss).sum()\n",
    "    value_loss = torch.stack(value_loss).sum()\n",
    "    policy_loss.backward()\n",
    "    value_loss.backward()\n",
    "    optimizer.step()\n",
    "    value_optimizer.step()\n",
    "    del model.rewards[:]\n",
    "    del model.saved_actions[:]\n",
    "    del value_model.state_values[:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(episodes):\n",
    "    \"\"\"\n",
    "    Train the policy network.\n",
    "    \"\"\"\n",
    "    running_reward = 10\n",
    "\n",
    "    # Keep track of the rewards for plotting\n",
    "    reward_history = []\n",
    "\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        ep_reward = 0\n",
    "\n",
    "        for t in range(10000):  # Don't infinite loop while learning\n",
    "\n",
    "            # Select action\n",
    "            action, log_prob, state_value = select_action(state)\n",
    "\n",
    "            # Take action\n",
    "            state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Save reward and state value\n",
    "            model.rewards.append(reward)\n",
    "            value_model.state_values.append(state_value)\n",
    "\n",
    "            ep_reward += reward\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "        reward_history.append(running_reward)\n",
    "        finish_episode()\n",
    "        if episode % log_interval == 0:\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tRunning reward: {:.2f}'.format(episode, ep_reward, running_reward))\n",
    "        if running_reward > env.spec.reward_threshold:\n",
    "            print(\"Solved! Running reward is now {} and the last episode ({}) runs to {} time steps!\".format(running_reward, episode, t))\n",
    "            break\n",
    "\n",
    "    return reward_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_history = train(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the reward history\n",
    "\n",
    "plt.plot(reward_history)\n",
    "plt.title('Reward history')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.legend(['Reward'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the policy using total regret\n",
    "\n",
    "def total_regret(episodes):\n",
    "    \"\"\"\n",
    "    Evaluate the policy using total regret.\n",
    "    \"\"\"\n",
    "    total_regret = 0\n",
    "\n",
    "    # Use reward history to calculate total regret\n",
    "    for reward in reward_history:\n",
    "        total_regret += env.spec.reward_threshold - reward\n",
    "\n",
    "    return total_regret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning\n",
    "\n",
    "# Set the hyperparameters\n",
    "hyperparameters = {\n",
    "    'lr': [1e-2, 1e-3, 1e-4],\n",
    "    'num_h_nodes': [16, 32, 64]\n",
    "}\n",
    "\n",
    "log_interval = 100\n",
    "episodes = 2000\n",
    "best_total_regret = float('inf')\n",
    "best_hyperparameters = {}\n",
    "regret_storage = []\n",
    "reward_history_storage = np.zeros((len(hyperparameters['lr']), len(hyperparameters['num_h_nodes']), episodes))\n",
    "\n",
    "# Loop through the hyperparameters\n",
    "for lr in hyperparameters['lr']:\n",
    "    for num_h_nodes in hyperparameters['num_h_nodes']:\n",
    "        \n",
    "        # Set the hyperparameters\n",
    "        model = Policy()\n",
    "        value_model = ValueFunction()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        value_optimizer = optim.Adam(value_model.parameters(), lr=lr)\n",
    "\n",
    "        # Train the model\n",
    "        reward_history = train(episodes)\n",
    "        #Fill the rest with env.spec.reward_threshold\n",
    "        reward_history += [env.spec.reward_threshold] * (episodes - len(reward_history))\n",
    "        reward_history_storage[hyperparameters['lr'].index(lr), hyperparameters['num_h_nodes'].index(num_h_nodes), :] = reward_history\n",
    "\n",
    "        # Calculate the total regret\n",
    "        total_regret = total_regret(episodes)\n",
    "\n",
    "        # Store the total regret\n",
    "        regret_storage.append([lr, num_h_nodes, total_regret])\n",
    "\n",
    "        # Check if this is the best total regret\n",
    "        if total_regret < best_total_regret:\n",
    "            best_total_regret = total_regret\n",
    "            best_hyperparameters = {'lr': lr, 'num_h_nodes': num_h_nodes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the best hyperparameters\n",
    "print('Best hyperparameters:', best_hyperparameters)\n",
    "print('Best total regret:', best_total_regret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the reward history for the best hyperparameters\n",
    "\n",
    "plt.plot(reward_history_storage[hyperparameters['lr'].index(best_hyperparameters['lr']), hyperparameters['num_h_nodes'].index(best_hyperparameters['num_h_nodes']), :])\n",
    "plt.title('Reward history for the best hyperparameters')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.legend(['lr: {}, num_h_nodes: {}'.format(best_hyperparameters['lr'], best_hyperparameters['num_h_nodes'])])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
